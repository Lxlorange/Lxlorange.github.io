---
title: 线性可分支持向量机
author: 凉香栾
date: 2025-10-29 09:16:43 +0800
categories:
  - 机器学习
tags:
  - 机器学习
  - SVM
description: 线性可分支持向量机是一种针对线性可分数据集的二类分类模型 。其核心思想是寻找一个间隔最大化的分离超平面，该间隔由距离超平面最近的“支持向量”所决定 。该模型的目标最终被形式化为一个凸二次规划问题 ，并通常通过拉格朗日对偶性和KKT条件来求解 。
toc: true
pin: false
math: true
mermaid: true
comment: true
---



线性可分支持向量机 (Linearly Separable SVM)又称为**硬间隔支持向量机**（Hard Margin SVM）。当训练数据线性可分时，它通过**间隔最大化**来学习一个线性的分类器。

所谓的间隔最大化，就是**几何间隔最大化**，也就是**硬间隔最大化**。

### 定义

给定通过间隔最大化求得的分类超平面为 $w^{\ast} \cdot x + b^{\ast} = 0$，以及相应的分类决策函数为$f(x)=sign(w^{\ast} \cdot{x} + b^\ast)$，称为线性可分支持向量机。

对于给定训练数据集$T$和超平面$(w,b)$，定义超平面关于样本点$(x_{i},y_{i})$的函数间隔为$\hat{\gamma_i} = y_i(w \cdot x_i + b)$ ，可以通过计算样本点与超平面的远见来表示分类的正确性和确信度。

但函数间隔有一个问题：如果将 $w$ 和 $b$ 成比例放大，超平面不变，但函数间隔会变为原来的若干倍。

因此可以通过约束法向量使得这个间隔确定下来，如规范化 $\Vert w \Vert=1$ ，使得间隔是确定的。因此可以定义几何间隔 $\gamma_i = y_i \left( \frac{w}{\Vert w \Vert} \cdot x_i + \frac{b}{\Vert w \Vert} \right) = \frac{\hat{\gamma_i}}{\Vert w \Vert}$ ，这是点到超平面的真实距离，它不随 $w$ 和 $b$ 的缩放而改变。

### 存在唯一性


可以证明，若训练数据集 $T$ 线性可分 ，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

### 间隔最大化

因此求得最大间隔分类超平面的约束最优化问题是$\max \limits_{w,b} \gamma$ ，考虑到函数间隔 $\hat{\gamma}$ 的改变对最大化几何间隔没有影响，则问题可以改写为：

$$
\begin{align}
 & \max\limits_{w,b} \frac{\hat{\gamma}}{\Vert w \Vert} \\
 & \text{s.t.} \quad y_i (w \cdot x_{i} + b) \ge \hat{\gamma}, i=1,2,\dots,N
\end{align}
$$


> [!NOTE] 
> 
> 凸二次规划问题 (Convex Quadratic Programming) 是凸优化问题的一种特定形式。
> 
> 凸优化问题指在一系列不等式约束$g_{i}(w) \leq 0$和等式约束$h_{i}(w)=0$下求解$\min\limits_{w} f(w)$
> 
> 当目标函数$f(w)$是一个二次函数，并且约束函数$g_{i}(w)$是一个仿射函数（线性函数加平移向量）时，这个问题就成为凸二次规划问题。

这个问题等价于最小化 $\Vert w \Vert$ ，令 $\hat{\gamma} = 1$，问题可以转换成凸二次规划问题。

$$
\begin{align}
 & \min\limits_{w,b} \frac{1}{2} \Vert w \Vert^2\\
 & \text{s.t.} \quad y_i (w \cdot x_{i} + b) -1 \geq 0, i=1,2,\dots,N
\end{align}
$$

> [!TIP]
> 机器学习中的凸函数指的是下凸上凹函数，例如$y=x^2$.

由此可以解得$w^{\ast} ,b^{\ast}$.

### 支持向量

在线性可分的情况下，训练数据集中与分离超平面**距离最近**的样本点被称为**支持向量**。这些点恰好位于间隔边界上。它们是使得约束条件 $y_i(w \cdot x_i + b) - 1 = 0$ 成立的点。

对于正例（$y_i = +1$），它们在 $H_1: w \cdot x + b = 1$ 上，对于负例（$y_i = -1$），它们在 $H_2: w \cdot x + b = -1$ 上。在决定分离超平面时，**只有支持向量在起作用**。其他实例点（非支持向量）即使被移除或在间隔边界外移动，解（即超平面）也不会改变。

分离超平面与$H_{1}$和$H_{2}$平行且位于它们中央。它们之间的距离称为间隔，宽度等于$\frac{2}{\Vert w \Vert}$，$H_{1}$和$H_{2}$称为间隔边界。

### 求解超平面

#### 消去法手动求解

例：

求 $w_{1,2} , b$，使得：

$$
\begin{align}
 & \min_{w,b} \frac{1}{2}(w_1^2 + w_2^2) \\
\text{s.t.} \quad & 3w_1 + 3w_2 + b \ge 1  \\
& 4w_1 + 3w_2 + b \ge 1  \\
& -w_1 - w_2 - b \ge 1
\end{align}
$$

解：

消去$b$，可以得到两个约束条件 $w_1 + w_2 \ge 1$ 和 $3w_1 + 2w_2 \ge 2$. 这意味着在 $w_1$-$w_2$ 平面上，找到一个满足这两个约束的点，使其离原点 $(0, 0)$ 最近。（因为最小化 $\frac{1}{2}(w_1^2 + w_2^2)$ 等价于最小化距离 $\sqrt{w_1^2 + w_2^2}$）。

这个离原点最近的点，必定落在可行域的**边界**上。边界由$L_A$: $w_1 + w_2 = 1$和$L_B$: $3w_1 + 2w_2 = 2$组成。

这个点要么在 $L_A$ 上，要么在 $L_B$ 上，要么在它们的交点上。利用线性规划的方法可以得出 $w_1 = 1/2$ 且 $w_2 = 1/2$。

代回约束，得到：
$$
1 - 3w_1 - 3w_2 \le b \le -1 - w_1 - w_2 \implies -2 \leq b \leq -2
$$

因此最终得到，$w_1 = 1/2$, $w_2 = 1/2$, $b = -2$，于是最大间隔分离超平面为$\frac{1}{2}x^{(1)} + \frac{1}{2}x^{(2)} - 2=0$

#### 拉格朗日对偶性的定义

直接求解原始的凸二次规划问题通常比较复杂，我们转而求解其**对偶问题**。

定义原始最优化问题为
$$
\begin{align}
 &  p^{\ast} = \min_{x} f(x)\\
 & \text{s.t. } c_i(x) \le 0, \quad h_j(x) = 0
\end{align}
$$

引入拉格朗日乘子 $\alpha_i \ge 0$ 和 $\beta_j$，构造广义拉格朗日函数：

$$L(x, \alpha, \beta) = f(x) + \sum \alpha_i c_i(x) + \sum \beta_j h_j(x)$$

这里$x = \left(x^{(1)},x^{(2)},\dots,x^{(n)}\right) \in R^n$，考虑$x$的函数$\theta_{p}(x) = \max\limits_{\alpha,\beta:\alpha_{i}\geq_{0}}L(x,\alpha,\beta)$，下标$p$表示原始问题。因此把 $\min\limits_{x} \max\limits_{\alpha, \beta: \alpha_i \ge 0} L(x, \alpha, \beta)$ 称为广义拉格朗日函数的**极小极大**问题，定义原始问题的（最优）值为 $p^{\ast} = \min\limits_{x}\theta_{p}(x)$.

为了得到对偶问题，我们需要定义拉格朗日对偶函数 (Lagrange Dual Function) 。它是广义拉格朗日函数关于原始变量求得的最小值：

$$
\theta_D(\alpha, \beta) = \min_x L(x, \alpha, \beta)
$$

对偶函数$\theta_D(\alpha, \beta)$ 是一个只关于对偶变量 $\alpha, \beta$ 的函数。自然，对偶问题是最大化对偶函数 $\theta_D(\alpha, \beta)$：

$$
\begin{align}
 & \max_{\alpha, \beta: \alpha_i \ge 0} \theta_D(\alpha, \beta) = \max_{\alpha, \beta: \alpha_i \ge 0} \min_x L(x, \alpha, \beta) \\
 & \text{s.t.} \quad \alpha_i \ge 0
\end{align}
$$

称为广义拉格朗日函数的**极大极小**问题，其（最优）值为 $d^{\ast} = \max\limits_{\alpha, \beta: \alpha_i \ge 0} \theta_D(\alpha, \beta)$.

#### 对偶问题的性质

弱对偶性说明，对于任何最优化问题，对偶问题的最优值 $d^{\ast}$ 永远是原始问题最优值 $p^{\ast}$ 的一个下界。即：$d^{\ast} \le p^{\ast}$。**这是显然的**。

强对偶性则说明，在满足特定条件时，对偶问题的最优值**等于**原始问题的最优值。即：$d^{\ast} = p^{\ast} = L(x^\ast,\alpha^*,\beta^\ast)$。

针对我们讨论得到这个凸二次规划问题，我们找到了一个特定条件，叫做 Slater 条件。Slater 条件是强对偶关系的一个充分条件。因为凸优化问题中每个约束都是一个凸集， Slater 条件要求这些约束的交集不能是空的，也不能是“病态的”（例如相切状态）。它要求这个交集必须形成一个有 “内部” 的区域。

具体可以表述为，不等式约束$c_{i}(x)$是严格可行的，即存在 $x$ ，对所有 $i$ 有 $c_{i}(x)<0$。 

Slater 条件对强对偶性的充分性可以证明。而同样可以证明，本问题满足 Slater 条件。

#### KKT 条件

当强对偶性 ($d^{\ast} = p^{\ast}$) 成立时，原始问题和对偶问题的最优解 $x^{\ast} , \alpha^{\ast} , \beta^{\ast}$ 必须满足一系列被称为 **KKT (Karush-Kuhn-Tucker) 条件**的必要条件。

> [!TIP]
> 
> KKT 条件的拉格朗日函数 $L(x, \alpha, \beta) = f(x) + \sum \alpha_i c_i(x) + \dots$ 默认是为 $c_i(x) \le 0$ 的约束形式设计的（因为 $\alpha_i \ge 0$）。
> 
> 我们的SVM原始问题约束是 $y_i (w \cdot x_i + b) - 1 \ge 0$。为了将其变为KKT的标准形式，我们改写为 $1 - y_i (w \cdot x_i + b) \le 0$。因此 $c_i(x)$ 对应 $1 - y_i (w \cdot x_i + b)$。
> 
> 如果在构造 $L$ 时，坚持使用 $\ge 0$ 的约束，则拉格朗日函数的第二项应变为**负号**，即 $L = f(x) - \sum \alpha_i c_i(x)$。

KKT 条件有很多种形式，针对支持向量机问题以这种形式呈现：

$$\begin{cases}  & (1) \quad \nabla_x L(x^{\ast} , \alpha^{\ast} , \beta^{\ast} ) = 0 \\  & (2) \quad \alpha_i^{\ast} c_i(x^*) = 0 \\  & (3) \quad c_i(x^{\ast} ) \le 0 \\  & (4) \quad \alpha_i^{\ast} \ge 0 \\  & (5) \quad h_j(x^*) = 0 \end{cases}$$

（1） 原始变量 $x$ 对应 $w, b$，分别求偏导得$\nabla_w L = 0 \implies w^{\ast} = \sum \alpha_i^{\ast} y_i x_i$ 和 $\nabla_b L = 0 \implies \sum \alpha_i^{\ast} y_i = 0$ ，以此可以得到只关于$\alpha_{i}$的对偶问题。这个性质称为平稳性。

（2） 在线性可分支持向量机中，$c_i(x^{\ast} ) = 1 - y_i(w^{\ast} \cdot x_i + b^{\ast} ) \le 0$，则 $\alpha_i^{\ast} (1 - y_i(w^{\ast} \cdot x_i + b^{\ast} )) = 0$。如果 $\alpha_{i}^\ast > 0$（即 $x_i$ 是支持向量），则必有 $1 - y_i(w^{\ast} \cdot x_i + b^{\ast} ) = 0$，即 $y_i(w^{\ast} \cdot x_i + b^{\ast} ) = 1$。这说明该点恰好在间隔边界；如果 $1 - y_i(w^{\ast} \cdot x_i + b^{\ast} ) < 0$（即 $y_i(w^{\ast} \cdot x_i + b^{\ast} ) > 1$），说明该点在间隔边界之外（非支持向量），则必有 $\alpha_i^{\ast} = 0$。这个性质称为互补松弛性。

剩余三个是可行性约束，来自于问题定义。

#### 对偶法求解

接下来需要求解$\min_x L(x, \alpha, \beta)$，对 $w,b$ 求偏导并令其为$0$，得到$\nabla_w L = 0 \implies w = \sum \alpha_i y_i x_i$ 和 $\nabla_b L = 0 \implies \sum \alpha_i y_i = 0$.

在我们本节的具体问题中，拉格朗日函数为：

$$
\begin{align}
L(w, b, \alpha) &= \frac{1}{2} \Vert w \Vert^2 - \sum_{i=1}^N \alpha_i (y_i(w \cdot x_i + b) - 1) \\ \\
& = \frac{1}{2} ||w||^2 - \sum_{i=1}^N \alpha_i y_i (w \cdot x_i) - b \sum_{i=1}^N \alpha_i y_i + \sum_{i=1}^N \alpha_i \\
& = \frac{1}{2} \sum \sum \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum \sum \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum \alpha_i  \\
 & = -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i
\end{align}
$$


消去了$w,b$，并将这个最大化问题 $\max -f(x)$ 转换为等价的最小化问题 $\min f(x)$ 

$$
\begin{align}
 & \min_\alpha \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i=1}^N \alpha_i \\
 & \text{s.t.} \quad \sum_{i=1}^N \alpha_i y_i = 0 \\
 & \alpha_i \ge 0, \quad i=1, 2, \dots, N
\end{align}
$$


利用优化算法迭代多次，可得最优解 $\alpha^\ast = (\alpha_1^\ast, \alpha_2^\ast, \dots, \alpha_N^\ast)^T$。


根据KKT条件，$w^\ast = \sum_{i=1}^N \alpha_i^\ast y_i x_i$且可以选择 $\alpha^\ast$ 的一个正分量 $\alpha_j^\ast > 0$（这个 $j$ 对应的 $x_j$ 就是一个支持向量），利用互补松弛性，有$y_j(w^\ast \cdot x_j + b^\ast) - 1 = 0$，代入：

$$
y_j \left( \left( \sum_{i=1}^N \alpha_i^\ast y_i x_i \right) \cdot x_j + b^\ast \right) - 1 = 0
$$

利用 $y_j^2 = 1$，两边同乘以 $y_j$，解得 $b^\ast$：

$$
b^\ast = y_j - \sum_{i=1}^N \alpha_i^\ast y_i (x_i \cdot x_j)
$$

### 总结

线性可分支持向量机使用拉格朗日对偶法的完整求解过程如下。

**输入**：线性可分训练集 $T = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$，其中 $x_i \in \mathcal{X} = R^n, y_i \in \mathcal{Y} = \{-1, +1\}, i=1, 2, \dots, N$；

**输出**：分离超平面和分类决策函数。

**(1)** 构造并求解约束最优化问题：

$$
\begin{align}
 & \min_\alpha \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i=1}^N \alpha_i \\
 & \text{s.t.} \quad \sum_{i=1}^N \alpha_i y_i = 0\\
 & \alpha_i \ge 0, \quad i=1, 2, \dots, N
\end{align}
$$


求得最优解 $\alpha^\ast = (\alpha_1^\ast, \alpha_2^\ast, \dots, \alpha_N^\ast)^T$。

**(2)** 计算

$$w^\ast = \sum_{i=1}^N \alpha_i^\ast y_i x_i$$

并选择 $\alpha^\ast$ 的一个正分量 $\alpha_j^\ast > 0$，计算

$$b^\ast = y_j - \sum_{i=1}^N \alpha_i^\ast y_i (x_i \cdot x_j)$$

**(3)** 求得分离超平面

$$w^\ast \cdot x + b^\ast = 0$$

分类决策函数：

$$f(x) = sign(w^\ast \cdot x + b^\ast)$$



